{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOvIj5JPPq2IFo/0zAqOIkl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"59d3d5601bcf4bc7b2cfa849cac3040b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_acb35542ff754e228cd0f306d45be0d4","IPY_MODEL_c443c9cc5ed4427fb67eec367fb6b08e","IPY_MODEL_5cbf0f2b5a434dde8c071b0cc023bba2"],"layout":"IPY_MODEL_fc1a6f7e1dad4078894e3750080c855b"}},"acb35542ff754e228cd0f306d45be0d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e55a70d1d97c43b8b7562b49640ebfca","placeholder":"â€‹","style":"IPY_MODEL_53015340d4744630b300c603e2031fd1","value":"Map:â€‡100%"}},"c443c9cc5ed4427fb67eec367fb6b08e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a674105f8f70439b95f883139dbf523e","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ddaba3f9fee04862a215f1ac7e6f4c2d","value":10}},"5cbf0f2b5a434dde8c071b0cc023bba2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4c66d88acb1e41de8f3a4e514ebe0b9a","placeholder":"â€‹","style":"IPY_MODEL_2e959824fe0343b6a13208c3a99ca60a","value":"â€‡10/10â€‡[00:00&lt;00:00,â€‡80.58â€‡examples/s]"}},"fc1a6f7e1dad4078894e3750080c855b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e55a70d1d97c43b8b7562b49640ebfca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"53015340d4744630b300c603e2031fd1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a674105f8f70439b95f883139dbf523e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ddaba3f9fee04862a215f1ac7e6f4c2d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4c66d88acb1e41de8f3a4e514ebe0b9a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2e959824fe0343b6a13208c3a99ca60a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["Fine-tune GPT or GPT-2 for creative story generation."],"metadata":{"id":"YnMhBNw4ZVxv"}},{"cell_type":"code","source":["import json\n","import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n","from datasets import load_dataset, Dataset"],"metadata":{"id":"rEc7wPIfNBag"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the AI-generated stories dataset\n","with open(\"ai_generated_stories.json\", \"r\", encoding=\"utf-8\") as file:\n","    data = json.load(file)"],"metadata":{"id":"DjQteV6QNBXX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert JSON to Hugging Face dataset format\n","texts = [item[\"story\"] for item in data]\n","dataset = Dataset.from_dict({\"text\": texts})"],"metadata":{"id":"1cRFL9PKNBUv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load GPT-2 Tokenizer\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.pad_token = tokenizer.eos_token  # Set pad token as EOS token"],"metadata":{"id":"lippc98cNJEX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Tokenize the dataset\n","def tokenize_function(examples):\n","    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n","\n","tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n","\n","# Load GPT-2 Model\n","model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n"],"metadata":{"id":"QlzR6gTJNJA1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training Arguments\n","training_args = TrainingArguments(\n","    output_dir=\"./gpt2-story-model\",\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    per_device_train_batch_size=2,\n","    per_device_eval_batch_size=2,\n","    num_train_epochs=3,\n","    save_total_limit=2,\n","    logging_dir=\"./logs\",\n","    logging_steps=500,\n","    report_to=\"none\"\n",")\n","\n","# Data Collator (for padding)\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False\n",")\n","\n","# Trainer Initialization\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_dataset,\n","    eval_dataset=tokenized_dataset,\n","    tokenizer=tokenizer,\n","    data_collator=data_collator\n",")\n","\n","# Train the model\n","trainer.train()\n","\n","# Save the fine-tuned model\n","model.save_pretrained(\"./fine_tuned_gpt2_story\")\n","tokenizer.save_pretrained(\"./fine_tuned_gpt2_story\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":395,"referenced_widgets":["59d3d5601bcf4bc7b2cfa849cac3040b","acb35542ff754e228cd0f306d45be0d4","c443c9cc5ed4427fb67eec367fb6b08e","5cbf0f2b5a434dde8c071b0cc023bba2","fc1a6f7e1dad4078894e3750080c855b","e55a70d1d97c43b8b7562b49640ebfca","53015340d4744630b300c603e2031fd1","a674105f8f70439b95f883139dbf523e","ddaba3f9fee04862a215f1ac7e6f4c2d","4c66d88acb1e41de8f3a4e514ebe0b9a","2e959824fe0343b6a13208c3a99ca60a"]},"id":"OK3jnRcJIXkP","executionInfo":{"status":"ok","timestamp":1741108911103,"user_tz":-330,"elapsed":663876,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"6c18334a-57eb-463f-f203-6ff06c491471"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Map:   0%|          | 0/10 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59d3d5601bcf4bc7b2cfa849cac3040b"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n","  warnings.warn(\n","<ipython-input-11-be1eb672ec79>:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [15/15 10:10, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>No log</td>\n","      <td>2.985490</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>No log</td>\n","      <td>2.671569</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>No log</td>\n","      <td>2.559867</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["('./fine_tuned_gpt2_story/tokenizer_config.json',\n"," './fine_tuned_gpt2_story/special_tokens_map.json',\n"," './fine_tuned_gpt2_story/vocab.json',\n"," './fine_tuned_gpt2_story/merges.txt',\n"," './fine_tuned_gpt2_story/added_tokens.json')"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["from transformers import pipeline, GPT2Tokenizer, GPT2LMHeadModel\n","\n","# Load the fine-tuned model and tokenizer\n","model_path = \"./fine_tuned_gpt2_story\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n","model = GPT2LMHeadModel.from_pretrained(model_path)\n","\n","# Initialize text generation pipeline\n","story_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n","\n","# Test prompt\n","# New test prompt\n","prompt = \"Deep within the enchanted forest, a hidden portal shimmered under the moonlight.\"\n","\n","# Generate a new story\n","generated_story = story_generator(prompt, max_length=250, num_return_sequences=1)\n","\n","# Print the output\n","print(\"Generated Story:\\n\")\n","print(generated_story[0][\"generated_text\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"emotDLKJI3kQ","executionInfo":{"status":"ok","timestamp":1741109202908,"user_tz":-330,"elapsed":29051,"user":{"displayName":"KOMAL JADHAV","userId":"16535022347121564782"}},"outputId":"1ec334a6-24f1-4308-faa9-309693bcf173"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n","Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"]},{"output_type":"stream","name":"stdout","text":["Generated Story:\n","\n","Deep within the enchanted forest, a hidden portal shimmered under the moonlight. It was full of mystical power and a twisted message in its hold told of a mysterious king who had cursed the people. Only those who knew the king would find it â€“ and they feared the message wouldn't keep them hidden.\n","\n","Soon, they stumbled across a massive chamber filled with strange objects inside. It haunted themâ€¦but their first clue came when they stopped on the trailâ€¦\n","\n","They stoppedâ€¦before a powerful demonic force turned them into something that could only hold their hearts. Summoning their own forces, they set the place on the edge of the forest, and watched for any sign of what was inside. However, a terrible warning soon appearedâ€¦\n","\n","â€¦and soon, there wasn't anything we could do.\n","\n","The door to the kingdom came undoneâ€¦when the king locked itâ€¦and whisperedâ€¦...\n","\n","â€¦inâ€¦the nightâ€¦for the last timeâ€¦\n","\n","â€¦neverâ€¦toldâ€¦\n","\n","A strange, powerful voice told themâ€¦a taleâ€¦andâ€¦they couldn't believe it. They didn't know it, of course, and just as they opened the door, a large force vanishedâ€¦\n","\n","â€¦\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"CF0WEohmLv24"},"execution_count":null,"outputs":[]}]}